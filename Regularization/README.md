# Regularization Techniques

This repository contains my implementations and experiments with **Regularization methods** in Machine Learning.  
Regularization helps to **reduce overfitting** by penalizing large coefficients and improving generalization on unseen data.

---

## ðŸ“˜ Ridge Regression (L2 Regularization)
###  1. Simple Linear Ridge Regression
- Implemented using the closed-form solution.  
- Works on a single feature dataset.  
- Shows how ridge regression prevents overfitting by shrinking coefficients.  


Ridge Regression adds an **L2 penalty** (squared magnitude of coefficients) to the loss function: 

<img src = "https://latex.codecogs.com/png.image?\huge&space;\dpi{110}\bg{white}L=\sum(y_{i}-\hat{y_{i}})^{2}&plus;\lambda(m)^{2}">

#### Coefficients m and b for 2D data :
<img src = "https://latex.codecogs.com/png.image?\huge&space;\dpi{110}\bg{white}&space;m=\frac{\sum((y_{i}-y_{mean})(x_{i}-x_{mean}))}{\sum(x_{i}-x_{mean})^{2}&plus;\lambda}">

<img src= "https://latex.codecogs.com/png.image?\huge&space;\dpi{110}\bg{white}b=\bar{y}-m\bar{x}">

### ðŸ”¹ 2. Multiple Linear Ridge Regression
- Implemented using the **OLS closed-form solution** with Ridge penalty.  

### ðŸš€ Ridge Regression (Gradient Descent)
This version focuses only on learning weights and bias without extra tracking.  

### ðŸ”— Lasso Regression  
Implementation of **Lasso Regression** from scratch using Python and NumPy.  
Introduces **L1 regularization**, which encourages sparsity by driving some coefficients exactly to zero.  

#### ðŸ“Š Steps Explanation:
To understand the derivation and intuition, check the diagrams below:

![Step 1](https://drive.google.com/uc?id=1M-zppzw9IO-rtkTWJyb_y_JXJsoQpgw8)
![Step 2](https://drive.usercontent.google.com/download?id=1LvT6rOYXrTu-klzX29CjD6ammtKZUh_p&authuser=0)


*(These images explain the mathematical derivation of Multiple Ridge Regression)*
#### Coefficients m and b for ND data:

here the only difference in the OLS Solution of linear regression and ridge regression is that Lambda is added into denominator because while differenciating the loss function with penalty term <strong>Î» Â· mÂ²</stromg>

- **Why use it?**
  - Reduces model complexity
  - Shrinks coefficients but does not eliminate them
  - Works well when we have multicollinearity

- **Hyperparameter**:  
  - `alpha` â†’ Controls strength of penalty  
    - High `alpha` â†’ more shrinkage, simpler model  
    - Low `alpha` â†’ behaves like Linear Regression  

---
## ðŸ“‚ Upcoming Additions
- Elastic Net Regression  

---
## ðŸš€ How to Run
1. Clone the repo:
   ```bash
   git clone https://github.com/Pruthil-2910/ML-From-Scratch.git
   cd ML-From-Scratch
